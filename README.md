# ğŸš€ Bitcoin ETL Data Pipeline

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PySpark](https://img.shields.io/badge/PySpark-3.x-orange.svg)
![Databricks](https://img.shields.io/badge/Databricks-Platform-red.svg)
![Delta Lake](https://img.shields.io/badge/Delta%20Lake-2.0+-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

**Production-ready Data Engineering project that builds an automated ETL pipeline to ingest real-time cryptocurrency and FX data, process it at scale, and deliver analytics-ready data for dashboards.**

Built with **Python**, **PySpark**, **Databricks**, **Unity Catalog**, and **Delta Lake**.

[ğŸ“– Documentation](#-about-the-project) â€¢ [ğŸš€ Quick Start](#-installation-and-setup) â€¢ [ğŸ—ï¸ Architecture](#ï¸-architecture) â€¢ [ğŸ“Š Examples](#-sql-query-examples)

</div>

---

## ğŸ“‹ Table of Contents

- [ğŸ“– About the Project](#-about-the-project)
- [âœ¨ Features](#-features)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸ¯ Business Use Case](#-business-use-case)
- [ğŸ”„ ETL Workflow](#-etl-workflow)
- [ğŸ“Š Data Model](#-data-model)
- [ğŸ› ï¸ Prerequisites](#ï¸-prerequisites)
- [ğŸš€ Installation and Setup](#-installation-and-setup)
- [ğŸ’» Usage](#-usage)
- [ğŸ“ˆ SQL Query Examples](#-sql-query-examples)
- [âš™ï¸ Automation and Orchestration](#ï¸-automation-and-orchestration)
- [ğŸ” Security and Best Practices](#-security-and-best-practices)
- [ğŸ“‚ Repository Structure](#-repository-structure)
- [ğŸ§° Tech Stack](#-tech-stack)
- [ğŸ§  Skills Demonstrated](#-skills-demonstrated)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“ License](#-license)

---

## ğŸ“– About the Project

This project implements a complete and automated ETL pipeline for ingesting, processing, and storing cryptocurrency (Bitcoin) and exchange rate data. The pipeline is designed to be scalable, reliable, and production-ready, utilizing modern data engineering best practices.

### ğŸ” Project Summary (TL;DR)

- ğŸ”„ **End-to-End ETL Pipeline**: Automated extraction, transformation, and loading
- ğŸŒ **Real-time API Ingestion**: Up-to-date data from public APIs
- âš™ï¸ **Automated with Databricks Workflows**: Scheduled or manual execution
- ğŸ§± **ACID-compliant Delta Tables**: Guaranteed consistency and integrity
- ğŸ“Š **SQL Analytics Layer**: Ready for dashboards and BI
- â˜ï¸ **Cloud-native Architecture**: Scalable and resilient

---

## âœ¨ Features

- âœ… **Real-time Ingestion**: Captures Bitcoin and exchange rate data via APIs
- âœ… **Distributed Processing**: Uses PySpark for large-scale processing
- âœ… **Multi-format Storage**: Supports JSON, Parquet, and Delta Tables
- âœ… **Time Travel**: Access historical data versions with Delta Lake
- âœ… **Schema Evolution**: Automatic schema evolution without breaking pipelines
- âœ… **Data Governance**: Unity Catalog for centralized management
- âœ… **ACID Transactions**: Guaranteed transactional consistency
- âœ… **Monitoring**: Execution logs and metrics

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Sources                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Coinbase API          â”‚    CurrencyFreaks API             â”‚
â”‚  (Bitcoin Price USD)   â”‚    (USD â†’ BRL Exchange Rate)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                       â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PySpark ETL Processing                         â”‚
â”‚  â€¢ Data Extraction                                          â”‚
â”‚  â€¢ Currency Conversion (USD â†’ BRL)                          â”‚
â”‚  â€¢ Data Transformation & Enrichment                         â”‚
â”‚  â€¢ Schema Standardization                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Delta Lake (Unity Catalog)                           â”‚
â”‚  â€¢ Catalog: pipeline_api_bitcoin                            â”‚
â”‚  â€¢ Schema: biticoin_delta                                   â”‚
â”‚  â€¢ Table: btc_data                                          â”‚
â”‚  â€¢ Volume: raw_files (JSON/Parquet)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SQL Analytics Layer                            â”‚
â”‚  â€¢ Real-time Queries                                        â”‚
â”‚  â€¢ Historical Analysis                                      â”‚
â”‚  â€¢ Dashboard Integration                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Main Components

1. **Extraction (Extract)**
   - Coinbase API: Bitcoin price in USD
   - CurrencyFreaks API: USD â†’ BRL exchange rate

2. **Transformation (Transform)**
   - Currency conversion (USD â†’ BRL)
   - Data type normalization
   - Timestamp enrichment
   - Schema standardization

3. **Loading (Load)**
   - Multi-format storage:
     - **JSON**: Human-readable raw data
     - **Parquet**: Optimized columnar format
     - **Delta Table**: ACID-compliant format with time travel

4. **Analytics**
   - Direct SQL queries on Delta Tables
   - Integration with BI tools
   - Real-time dashboards

---

## ğŸ¯ Business Use Case

This pipeline was developed to meet the following business needs:

- ğŸ“ˆ **Trend Tracking**: Monitor Bitcoin prices in Brazilian Real (BRL)
- ğŸ“š **Complete History**: Maintain historical price data with full auditability
- âš¡ **Real-time Monitoring**: Enable real-time analysis and alerts
- ğŸ“Š **Advanced Analytics**: Support complex analysis and executive dashboards
- ğŸ”„ **Scalability**: Designed to scale from small workloads to Big Data

---

## ğŸ”„ ETL Workflow

### 1ï¸âƒ£ Extract (Extraction)

**Data Sources:**
- **Bitcoin Price (USD)**: Extracted from [Coinbase API](https://api.coinbase.com/v2/prices/spot)
- **USD â†’ BRL Exchange Rate**: Extracted from [CurrencyFreaks API](https://api.currencyfreaks.com/)

**Features:**
- Robust API handling (JSON parsing, error handling)
- Retry logic for resilience
- Data validation on ingestion

### 2ï¸âƒ£ Transform (Transformation)

**Operations Performed:**
- âœ… Currency conversion (USD â†’ BRL)
- âœ… Data type normalization
- âœ… Timestamp enrichment
- âœ… Schema standardization
- âœ… Data validation

**Transformation Example:**
```python
valor_brl = valor_usd * taxa_usd_brl
timestamp = datetime.now()
```

### 3ï¸âƒ£ Load (Loading)

**Storage Destinations:**

1. **Raw Volume (JSON/Parquet)**
   - Location: `/Volumes/pipeline_api_bitcoin/lakehouse/raw_files/`
   - Format: JSON and Parquet
   - Purpose: Raw data for auditing and reprocessing

2. **Delta Table**
   - Location: `pipeline_api_bitcoin.biticoin_delta.btc_data`
   - Format: Delta Lake
   - Mode: Incremental append (historical tracking)
   - Schema Evolution: Enabled

**Features:**
- âœ… Incremental append for complete history
- âœ… Schema evolution enabled
- âœ… ACID transactions guaranteed
- âœ… Time travel available

---

## ğŸ“Š Data Model

### Delta Table Schema: `btc_data`

| Column | Type | Description |
|--------|------|-------------|
| `valor_usd` | `DOUBLE` | Bitcoin price in USD |
| `valor_brl` | `DOUBLE` | Bitcoin price in BRL (calculated) |
| `criptomoeda` | `STRING` | Cryptocurrency code (BTC) |
| `moeda_original` | `STRING` | Original price currency (USD) |
| `taxa_conversao_usd_brl` | `DOUBLE` | USD â†’ BRL exchange rate used |
| `timestamp` | `TIMESTAMP` | Ingestion timestamp |

### Record Example

```json
{
  "valor_usd": 43250.50,
  "valor_brl": 215000.00,
  "criptomoeda": "BTC",
  "moeda_original": "USD",
  "taxa_conversao_usd_brl": 4.97,
  "timestamp": "2024-01-15T10:30:45.123456"
}
```

---

## ğŸ› ï¸ Prerequisites

Before getting started, make sure you have:

- âœ… **Databricks Workspace**: Access to a Databricks workspace (Community Edition or higher)
- âœ… **Python 3.8+**: Compatible Python version
- âœ… **Coinbase API Access**: Public access (no authentication required)
- âœ… **CurrencyFreaks API Key**: API key for CurrencyFreaks (get it at [currencyfreaks.com](https://currencyfreaks.com))
- âœ… **Python Libraries**:
  - `requests`
  - `pandas`
  - `pyspark` (provided by Databricks)

---

## ğŸš€ Installation and Setup

### 1. Clone the Repository

```bash
git clone <repository-url>
cd ETL_Bitcoin
```

### 2. Configure Databricks

1. Access your Databricks workspace
2. Create a new notebook or import the existing notebook
3. Attach the notebook to a Databricks cluster

### 3. Configure APIs

#### CurrencyFreaks API Key

âš ï¸ **Important**: Never hardcode API keys in code!

**Option 1: Use Databricks Secrets (Recommended)**
```python
# In Databricks, configure the secret:
# Databricks CLI: databricks secrets create-scope --scope bitcoin-pipeline
# databricks secrets put --scope bitcoin-pipeline --key currencyfreaks-api-key

# In code:
api_key = dbutils.secrets.get(scope="bitcoin-pipeline", key="currencyfreaks-api-key")
```

**Option 2: Environment Variables**
```python
import os
api_key = os.environ.get('CURRENCYFREAKS_API_KEY')
```

### 4. Configure Unity Catalog

Execute the SQL cells in the notebook to create the infrastructure:

```sql
-- Create Catalog
CREATE CATALOG IF NOT EXISTS pipeline_api_bitcoin
COMMENT 'Catalog for pipeline api Bitcoin';

-- Create Schema for Delta Tables
CREATE SCHEMA IF NOT EXISTS pipeline_api_bitcoin.biticoin_delta
COMMENT 'Lakehouse schema to store delta data';

-- Create Schema for Lakehouse
CREATE SCHEMA IF NOT EXISTS pipeline_api_bitcoin.lakehouse
COMMENT 'Lakehouse schema to store processed data';

-- Create Volume for raw files
CREATE VOLUME IF NOT EXISTS pipeline_api_bitcoin.lakehouse.raw_files
COMMENT 'Volume for raw files from initial ingestion';
```

### 5. Install Dependencies

In the Databricks notebook, execute:

```python
%pip install requests pandas
```

---

## ğŸ’» Usage

### Manual Execution

1. Open the notebook `src/Get_Biticoin.ipynb` in Databricks
2. Execute all cells sequentially
3. Data will be extracted, transformed, and loaded automatically

### Workflow Execution (Recommended)

1. **Create Job in Databricks**:
   - Go to **Workflows** â†’ **Create Job**
   - Add the notebook as a task
   - Configure parameters if needed

2. **Configure Schedule** (Optional):
   - Set up a cron schedule (e.g., `0 */1 * * *` to run every hour)
   - Or execute manually when needed

3. **Monitor Executions**:
   - Track logs and metrics in the Databricks interface
   - Set up alerts for failures

### Main Code Example

```python
# 1. Extract data
dados_bitcoin = Extract_Bitcoin_Data()
dados_cotacao = extrair_cotacao_usd_brl()

# 2. Extract conversion rate
taxa_usd_brl = float(dados_cotacao['rates']['BRL'])

# 3. Transform data
dados_bitcoin_tratado = tratar_dados_bitcoin(dados_bitcoin, taxa_usd_brl)

# 4. Create DataFrame
df_bitcoin = pd.DataFrame(dados_bitcoin_tratado)

# 5. Save in multiple formats
# JSON and Parquet (raw files)
# Delta Table (analytics-ready)
```

---

## ğŸ“ˆ SQL Query Examples

### Latest Bitcoin Price

```sql
SELECT 
    valor_usd,
    valor_brl,
    timestamp
FROM pipeline_api_bitcoin.biticoin_delta.btc_data
ORDER BY timestamp DESC
LIMIT 1;
```

### Historical Maximum Price

```sql
SELECT 
    MAX(valor_brl) AS preco_maximo_brl,
    MAX(valor_usd) AS preco_maximo_usd,
    MAX(timestamp) AS data_maximo
FROM pipeline_api_bitcoin.biticoin_delta.btc_data;
```

### Historical Minimum Price

```sql
SELECT 
    MIN(valor_brl) AS preco_minimo_brl,
    MIN(valor_usd) AS preco_minimo_usd,
    MIN(timestamp) AS data_minimo
FROM pipeline_api_bitcoin.biticoin_delta.btc_data;
```

### Temporal Evolution (Last 24 Hours)

```sql
SELECT 
    DATE_TRUNC('hour', timestamp) AS hora,
    AVG(valor_brl) AS preco_medio_brl,
    AVG(valor_usd) AS preco_medio_usd,
    COUNT(*) AS registros
FROM pipeline_api_bitcoin.biticoin_delta.btc_data
WHERE timestamp >= CURRENT_TIMESTAMP - INTERVAL 24 HOURS
GROUP BY DATE_TRUNC('hour', timestamp)
ORDER BY hora DESC;
```

### Volatility Analysis

```sql
SELECT 
    DATE(timestamp) AS data,
    MIN(valor_brl) AS minimo_brl,
    MAX(valor_brl) AS maximo_brl,
    AVG(valor_brl) AS media_brl,
    (MAX(valor_brl) - MIN(valor_brl)) / AVG(valor_brl) * 100 AS volatilidade_percentual
FROM pipeline_api_bitcoin.biticoin_delta.btc_data
GROUP BY DATE(timestamp)
ORDER BY data DESC
LIMIT 30;
```

### Time Travel - Access Previous Version

```sql
-- View version history
DESCRIBE HISTORY pipeline_api_bitcoin.biticoin_delta.btc_data;

-- Access specific version (e.g., version 5)
SELECT * FROM pipeline_api_bitcoin.biticoin_delta.btc_data 
VERSION AS OF 5;
```

---

## âš™ï¸ Automation and Orchestration

### Databricks Workflows

The pipeline can be automated using Databricks Jobs & Workflows:

**Features:**
- âœ… Parameterized execution (API Keys via secrets)
- âœ… Manual or scheduled execution (cron)
- âœ… Execution logs and monitoring
- âœ… Automatic retry on failure
- âœ… Email/Slack notifications

**Job Configuration Example:**

```json
{
  "name": "Bitcoin ETL Pipeline",
  "schedule": {
    "quartz_cron_expression": "0 */1 * * * ?",
    "timezone_id": "America/Sao_Paulo"
  },
  "tasks": [
    {
      "task_key": "extract_transform_load",
      "notebook_task": {
        "notebook_path": "/src/Get_Biticoin",
        "base_parameters": {}
      }
    }
  ]
}
```

---

## ğŸ” Security and Best Practices

### Security

- âœ… **API Keys never hardcoded**: Always use Databricks Secrets or environment variables
- âœ… **Parameterized secrets in workflows**: Secure credential configuration
- âœ… **Data governance via Unity Catalog**: Centralized access control
- âœ… **ACID-compliant storage**: Guaranteed transactional consistency
- âœ… **Time travel for auditing**: Complete change traceability

### Implemented Best Practices

1. **Separation of Concerns**: Modular and reusable functions
2. **Error Handling**: Robust validation and exception handling
3. **Logging**: Detailed operation logging
4. **Schema Evolution**: Support for schema changes without breaking pipelines
5. **Incremental Loading**: Append mode for efficiency and complete history
6. **Multi-format Storage**: Flexibility for different use cases

---

## ğŸ“‚ Repository Structure

```
ETL_Bitcoin/
â”‚
â”œâ”€â”€ README.md                 # Main documentation
â”‚
â””â”€â”€ src/
    â””â”€â”€ Get_Biticoin.ipynb   # Main ETL pipeline notebook
        â”‚
        â”œâ”€â”€ 1. Import Libraries
        â”œâ”€â”€ 2. Extract and Transform Data
        â”‚   â”œâ”€â”€ Extract_Bitcoin_Data()
        â”‚   â”œâ”€â”€ extrair_cotacao_usd_brl()
        â”‚   â””â”€â”€ tratar_dados_bitcoin()
        â”‚
        â”œâ”€â”€ 3. Config Unity Catalog
        â”‚   â”œâ”€â”€ Create Catalog
        â”‚   â”œâ”€â”€ Create Schemas
        â”‚   â””â”€â”€ Create Volume
        â”‚
        â”œâ”€â”€ 4. Create Pandas DataFrame
        â”œâ”€â”€ 5. Save to JSON
        â”œâ”€â”€ 6. Save to Parquet
        â”œâ”€â”€ 7. Convert to PySpark DataFrame
        â”œâ”€â”€ 8. Save as Delta Table
        â”œâ”€â”€ 9. Read Delta Table
        â”œâ”€â”€ 10. Query with SQL
        â””â”€â”€ 11. Check Delta History (Time Travel)
```

---

## ğŸ§° Tech Stack

| Technology | Version | Purpose |
|------------|---------|---------|
| **Python** | 3.8+ | Main programming language |
| **PySpark** | 3.x | Distributed data processing |
| **Pandas** | Latest | In-memory data manipulation |
| **Requests** | Latest | HTTP client for APIs |
| **Databricks** | Platform | Processing and orchestration platform |
| **Delta Lake** | 2.0+ | ACID-compliant storage format |
| **Unity Catalog** | Latest | Data governance and catalog |

### Format Comparison

| Format | Use Case | Advantages | Disadvantages |
|--------|----------|------------|---------------|
| **CSV** | Debugging, small datasets | Readable, universal | Large size, slow |
| **JSON** | Raw data, APIs | Readable, flexible | Large size, not optimized |
| **Parquet** | Big Data, analytics | Compact, fast, columnar | Binary, not readable |
| **Delta Table** | Data Warehouse, critical pipelines | ACID, time travel, schema evolution | Requires Delta support |

---

## ğŸ§  Skills Demonstrated

This project demonstrates proficiency in:

- âœ… **Data Engineering**: Building robust ETL pipelines
- âœ… **ETL/ELT Pipelines**: Data extraction, transformation, and loading
- âœ… **PySpark & Distributed Processing**: Large-scale distributed processing
- âœ… **Delta Lake & Data Lakehouse**: Modern data architecture
- âœ… **Databricks Workflows**: Automation and orchestration
- âœ… **SQL Analytics**: Analytical queries on structured data
- âœ… **API Integration**: REST API integration
- âœ… **Cloud Data Architecture**: Scalable cloud-native architecture
- âœ… **Data Governance**: Unity Catalog and data governance
- âœ… **Best Practices**: Security, versioning, and documentation

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:

1. ğŸ´ Fork the project
2. ğŸŒ¿ Create a branch for your feature (`git checkout -b feature/AmazingFeature`)
3. ğŸ’¾ Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. ğŸ“¤ Push to the branch (`git push origin feature/AmazingFeature`)
5. ğŸ”€ Open a Pull Request

### Contribution Guidelines

- Follow existing code standards
- Add tests for new features
- Update documentation as needed
- Keep commits descriptive and organized

---




